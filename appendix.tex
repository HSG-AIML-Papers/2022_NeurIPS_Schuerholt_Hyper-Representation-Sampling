\section{Model Zoo Details}
\label{app:zoos}
\begin{wraptable}{r}{0.55\linewidth}
\vspace{-6mm}
\small
\captionof{table}{
Model zoo overview.}\label{tab:zoo_overview}\vspace{2mm}
% \setlength{\tabcolsep}{6pt}
% \begin{tabularx}{\linewidth}{rccccc}
% \toprule
% \begin{table}[]
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Zoo} & \textbf{Input Channels} & \textbf{Parameters} & \textbf{Population Size} \\ 
\midrule
MNIST        & 1                       & 2464                & 1000                     \\
SVHN         & 1                       & 2464                & 1000                     \\
CIFAR-10     & 3                       & 2864                & 1000                     \\
STL-10       & 3                       & 2864                & 1000                     \\ 
\bottomrule
\end{tabular}
% \end{table}
% \end{tabularx}
\vspace{-4mm}
\end{wraptable} 
The model zoos are generated following the method of ~\citep{schurholtSelfSupervisedRepresentationLearning2021,schurholtModelZoosDataset2022}
An overview of the model zoos is given in in Table \ref{tab:zoo_overview}.
All model zoos share one general CNN architecture, outlined in Table \ref{tab:model_zoo_architecture}. 
The hyperparameter choices for each of the population are listed in Table \ref{tab:model_zoo_hyperparameters}. 
The hyperparameters are chosen to generate zoos with smooth, continuous development and spread in performance. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[ht!]
    \begin{minipage}{0.4\textwidth}
    \centering
    {\small
    \caption{CNN architecture details for the models in model zoos. }
    \begin{tabularx}{\linewidth}{lll}
        \toprule
        \textbf{Layer}          & \textbf{Component} & \textbf{Value} \\
        \cmidrule(r){1-1} \cmidrule(rl){2-2}  \cmidrule(rl){3-3}% \cmidrule(rl){6-6} \cmidrule(l){7-8}
        % \midrule 
        \multirow{5}{*}{Conv 1} & input channels     & 1/3                 \\
                                & output channels    & 8                    \\
                                & kernel size        & 5                    \\
                                & stride             & 1                    \\
                                & padding            & 0                    \\
        % \midrule
        \cmidrule(r){1-1} \cmidrule(rl){2-2}  \cmidrule(rl){3-3}% \cmidrule(rl){6-6} \cmidrule(l){7-8}
        Max Pooling             & kernel size        & 2                    \\
        % \midrule
        \cmidrule(r){1-1} \cmidrule(rl){2-2}  \cmidrule(rl){3-3}% \cmidrule(rl){6-6} \cmidrule(l){7-8}
        Activation              & tanh / gelu         &                      \\
        % \midrule
        \cmidrule(r){1-1} \cmidrule(rl){2-2}  \cmidrule(rl){3-3}% \cmidrule(rl){6-6} \cmidrule(l){7-8}
        \multirow{5}{*}{Conv 2} & input channels     & 8                    \\
                                & output channels    & 6                    \\
                                & kernel size        & 5                    \\
                                & stride             & 1                    \\
                                & padding            & 0                    \\
        % \midrule
        \cmidrule(r){1-1} \cmidrule(rl){2-2}  \cmidrule(rl){3-3}% \cmidrule(rl){6-6} \cmidrule(l){7-8}
        Max Pooling             & kernel size        & 2                    \\
        % \midrule
        \cmidrule(r){1-1} \cmidrule(rl){2-2}  \cmidrule(rl){3-3}% \cmidrule(rl){6-6} \cmidrule(l){7-8}
        Activation              & tanh / gelu         &                      \\
        % \midrule
        \cmidrule(r){1-1} \cmidrule(rl){2-2}  \cmidrule(rl){3-3}% \cmidrule(rl){6-6} \cmidrule(l){7-8}
        \multirow{5}{*}{Conv 3} & input channels     & 6                    \\
                                & output channels    & 4                    \\
                                & kernel size        & 2                    \\
                                & stride             & 1                    \\
                                & padding            & 0                    \\
        % \midrule
        \cmidrule(r){1-1} \cmidrule(rl){2-2}  \cmidrule(rl){3-3}% \cmidrule(rl){6-6} \cmidrule(l){7-8}
        Activation              & tanh / gelu         &                      \\
        % \midrule
        \cmidrule(r){1-1} \cmidrule(rl){2-2}  \cmidrule(rl){3-3}% \cmidrule(rl){6-6} \cmidrule(l){7-8}
        \multirow{2}{*}{Linear 1} & input channels     & 36                   \\
                                & output channels    & 20                   \\
        % \midrule
        \cmidrule(r){1-1} \cmidrule(rl){2-2}  \cmidrule(rl){3-3}% \cmidrule(rl){6-6} \cmidrule(l){7-8}
        Activation              & tanh / gelu         &                      \\
        % \midrule
        \cmidrule(r){1-1} \cmidrule(rl){2-2}  \cmidrule(rl){3-3}% \cmidrule(rl){6-6} \cmidrule(l){7-8}
        \multirow{2}{*}{Linear 2} & input channels     & 20                   \\
                                & output channels    & 10                   \\
        \bottomrule
    \end{tabularx}
    \label{tab:model_zoo_architecture}
    }
    \end{minipage}
    \hspace{2mm}
    \begin{minipage}{0.58\textwidth}
    \centering
    {\small
    \caption{Hyperparameter choices for the model zoos. }
    \begin{tabularx}{\linewidth}{lll}
        \toprule
        \textbf{Model Zoo}     & \textbf{Hyperparameter} & \textbf{Value}   \\
        % \midrule
        \cmidrule(r){1-1} \cmidrule(rl){2-2}  \cmidrule(rl){3-3}
        \multirow{5}{*}{MNIST} & input channels          & 1                \\
                               & activation              & tanh             \\
                               & weight decay                      & 0                \\
                               & learning rate                      & 3e-4             \\
                               & initialization          & uniform          \\
                               & optimizer               & Adam             \\
                               & seed                    & [1-1000]         \\
        % \midrule
        \cmidrule(r){1-1} \cmidrule(rl){2-2}  \cmidrule(rl){3-3}% \cmidrule(rl){6-6} \cmidrule(l){7-8}
        \multirow{5}{*}{SVHN}  & input channels          & 1                \\
                               & activation              & tanh             \\
                               & weight decay                      & 0                \\
                               & learning rate                      & 3e-3             \\
                               & initialization          & uniform          \\
                               & optimizer               & adam             \\
                               & seed                    & [1-1000]         \\
        % \midrule
        \cmidrule(r){1-1} \cmidrule(rl){2-2}  \cmidrule(rl){3-3}% \cmidrule(rl){6-6} \cmidrule(l){7-8}
        % \multirow{5}{*}{USPS}  & input channels          & 1                \\
        %                       & activation              & tanh             \\
        %                       & weight decay                      & 1e-3             \\
        %                       & learning rate                      & 1e-4             \\
        %                       & initialization          & kaiming\_uniform \\
        %                       & optimizer               & adam             \\
        %                       & seed                    & [1-1000]         \\
        % \midrule
        \multirow{5}{*}{CIFAR-10} & input channels          & 3                \\
                               & activation              & gelu             \\
                               & weight decay                      & 1e-2             \\
                               & learning rate                      & 1e-4             \\
                               & initialization          & kaiming-uniform  \\
                               & optimizer               & adam             \\
                               & seed                    & [1-1000]         \\
        % \midrule
        \cmidrule(r){1-1} \cmidrule(rl){2-2}  \cmidrule(rl){3-3}% \cmidrule(rl){6-6} \cmidrule(l){7-8}
        \multirow{5}{*}{STL-10}& input channels          & 3                \\
                               & activation              & tanh             \\
                               & weight decay                      & 1e-3             \\
                               & learning rate                      & 1e-4             \\
                               & initialization  a        & kaiming-uniform  \\
                               & optimizer               & adam             \\
                               & seed                    & [1-1000]         \\
        \bottomrule
    \end{tabularx}
    \label{tab:model_zoo_hyperparameters}

    
    }
    \end{minipage}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Hyper-Representation Architecture and Training Details}
\label{app:training}
\begin{figure}[ht!]
\begin{minipage}[t]{1.0\textwidth}
\begin{center}
% \includegraphics[trim=0in 0in 0in 0in, width=0.99\linewidth]{figures/weight_space_baseline.pdf}
\includegraphics[trim=0in 0in 0in 0in, width=0.99\linewidth]{figures/hyper_rep_architecture_v3.png}
\caption{
Schematic of the auto-encoder architecture to learn hyper-representations. 
}
\label{fig:autoencoder_architecture}    
\end{center}
\end{minipage}
\end{figure}
Hyper-representations are learned with an autoencoder based on multi-head self-attention. The architecture is outlined in Figure \ref{fig:autoencoder_architecture}. 
Convolutional and fully connected neurons are embedded to token embeddings of dimension $d_{token}$. 
Learned position encodings are added to provide relational information.
A learned compression token (CLS) is appended to the sequence of token embeddings. 
The sequence of token embeddings is passed to $N_{layers}$ layers of multi-head self-attention with $N_{heads}$ heads with hidden embedding dimension $d_{hidden}$. 
The CLS token is compressed to the bottleneck of dimension $d_{z}$ with an MLP or a linear layer.
For the decoder, an MLP or a linear layer maps the bottleneck to a sequence of token embeddings.
The sequence is passed through another stack of multi-head self-attention, which is symmetric to the encoder.
Debedders map the token embeddings back to convolutional and fully connected neurons.
The reconstruction and contrastive loss are balanced with a parameter $\beta$. The contrastive loss is computed on the embeddings $\mathbf{z}$ mapped through a projection head $\mathbf{\bar{z}}=p(\mathbf{z}$, where $p$ is a learned MLP with four layers with 400 neurons each and $\mathbf{\bar{z}}$ has 50 dimensions.
In Table \ref{tab:hyper_rep_training}, the exact hyper-parameters for each of the hyper-representation are listed to reproduce our results.

\begin{table}[ht!]
    \centering
    \begin{minipage}{0.7\linewidth}
    \centering
    {\small
    \caption{Hyper-representation architecture and training details. }
    \begin{tabularx}{\linewidth}{lcccc}
    % \centering
    \toprule
                        & \texttt{MNIST}    & \texttt{SVHN}     & \texttt{CIFAR-10} & \texttt{STL-10}   \\
    \cmidrule(r){2-2} \cmidrule(rl){3-3}  \cmidrule(rl){4-4} \cmidrule(rl){5-5}
                        & \multicolumn{4}{c}{\textit{Architecture}} \\
    \cmidrule(r){2-2} \cmidrule(rl){3-3}  \cmidrule(rl){4-4} \cmidrule(rl){5-5}
    $d_{inpot}$         & 2464     & 2464     & 2864     & 2864     \\
    $d_{token}$         & 972      & 1680     & 1488     & 1632     \\
    $d_{hidden}$        & 1140     & 1800     & 1164     & 1680     \\
    $N_{layers}$        & 2        & 4        & 2        & 4        \\
    $N_{heads}$         & 12       & 12       & 12       & 24       \\
    $d_{z}$             & 700      & 1000     & 700      & 700      \\
    Compression         & linear   & linear   & linear   & linear   \\
    \cmidrule(r){2-2} \cmidrule(rl){3-3}  \cmidrule(rl){4-4} \cmidrule(rl){5-5}
                        & \multicolumn{4}{c}{\textit{Training}} \\
    \cmidrule(r){2-2} \cmidrule(rl){3-3}  \cmidrule(rl){4-4} \cmidrule(rl){5-5}
    Optimizer           & Adam     & Adam     & Adam     & Adam     \\
    Learning rate       & 0.0001   & 0.0001   & 0.0001   & 0.0001   \\
    Dropout             & 0.1      & 0.1      & 0.1      & 0.1      \\
    Weight Decay        & 1e-09    & 1e-09    & 1e-09    & 1e-09    \\
    $\beta$             & 0.977    & 0.920    & 0.950    & 0.950    \\
    training epochs     & 1750     & 1750     & 500      & 2000     \\
    batch size          & 500      & 250      & 200      & 200     \\
    \bottomrule
    \end{tabularx}
    \label{tab:hyper_rep_training}
    }
    \end{minipage}
    
    
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Evaluation of Layer-Wise Loss Normalization }
\label{app:loss}
To evaluate layer-wise loss normalization, we compare two hyper-representations with comparable reconstruction. Both have a $R^2=1-\frac{mse(\hat{\mathbf{w}},\mathbf{w})}{mse(\mathbf{w_{mean},\mathbf{w}}}$ as a measure of the explained variance of around 70\%. One is trained trained with the baseline hyper-representation MSE, the other with layer-wise-normalization. 
\begin{figure}[ht!]
\begin{minipage}[t]{1.0\textwidth}
\begin{center}
\includegraphics[trim=0in 0in 0in 0in, width=0.8\linewidth]{figures/weight_distribution_accuracy_recon_raw.png}
\vskip -0.1in
\caption{
\textbf{Top:} Weight distribution per layer (1-5) of the SVHN test set before $w$ and after reconstruction $\hat{w}$ with the basline hyper-representation training loss. 
Layers 3 and 4 have small weight distributions, therefore add little penalty to the MSE and are consequently poorly reconstructed. 
\textbf{Bottom:} Accuracy distribution of the same population before and after reconstruction. 
The badly reconstructed layers (top) cause the reconstructed models to perform around random guessing.
}
\label{fig:weight_distro_raw}    
\end{center}
\end{minipage}
\vspace{2mm}
\begin{minipage}[t]{1.0\textwidth}
\begin{center}
\includegraphics[trim=0in 0in 0in 0in, width=0.8\linewidth]{figures/weight_distribution_accuracy_recon_norm.png}
\vskip -0.1in
\caption{
\textbf{Top:} Weight distribution per layer (1-5) of the SVHN test set before $w$ and after reconstruction $\hat{w}$ with layer-wise loss normalization.
The distributions of all layers are more similar, the reconstruction is equally distributed across the layers.
\textbf{Bottom:} Accuracy distribution of the same population before and after reconstruction. 
The normalization fixes the catastrophic failure of the models. The remaining loss in accuracy can be explained with remaining reconstruction error. 
}
\label{fig:weight_distro_norm}    
\end{center}
\end{minipage}
\end{figure}
Figures \ref{fig:weight_distro_raw} and \ref{fig:weight_distro_norm} show the distribution of weights per layer before and after reconstruction, as well as the accuracy distribution of both populations on the SVHN image test set. With the basline learning scheme in Figure \ref{fig:weight_distro_raw}, the distributions in layers 3 and 4 do not match. In these layers, the original weight distribution is smaller, and so there is only a small error even if the reconstructions predicts the mean. These layers become a weak link of the reconstructed models, and cause performance around random guessing.
With layer-wise loss normalization in Figure \ref{fig:weight_distro_norm}, the weight distribution between the layers becomes more similar. As a consequence, the reconstruction error is more evenly distributed across the layers, there are no single layers that aren't reconstructed at all.  
This appears to allow information to flow forward through the model, and significantly improves the performance of reconstructed models. 
We find layer-wise-normalization necessary to reconstruct or sample functional models across all populations, where the weights are unevenly distributed. 

%%%% Reconstruction Error %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% First, whether the training objective of hyper-representations lead to functional models.
% \paragraph{Performance loss and reconstruction loss correlate}
% Weights that are passed through encoder and decoder are not reconstructed perfectly. The remaining reconstruction error of the weights naturally hurts the performance of the model on its downstream image classification task.
% We observe two different modes of errors: 
% i) unequal distribution of weights leading to unequal reconstruction of the different errors. In these cases, at least one layer is essentially ignored.  Here, even small overall reconstruction errors cause performance loss up to random guessing; 
% ii) approximately equal distribution of weights over the layers. The reconstruction is equally accurate over the layers.
% %
% \begin{figure}%[t]
% \begin{minipage}[t]{0.5\textwidth}
% \begin{center}
% \includegraphics[trim=0in 0in 0in 0in, clip, width=0.95\linewidth]{figures/reconstruction_accuracy_diff_vs_weight_dist.png}
% \vskip -0.1in
% \caption{Difference between models before and after reconstruction of the MNIST zoo. Accuracy difference over distance in $\ell_2$ norm. 
% % Both develop approximately proportional: the lower the reconstruction error in the weights, the smaller the accuracy difference.
% }
% \label{fig:reconstruction_error_accuracy}    
% \end{center}
% \end{minipage}
% \end{figure}




% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%% Shape of Embeddings %%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Hyper-Representation Analysis}
\label{app:analysis}
In this section, we detail the analysis of hyper-representations. We begin with their geometry, followed by the distributions of individual dimensions of hyper-representations, and finally investigate robustness and smoothness.

\paragraph{Embeddings in Hyper-Representation Space Populate a Hyper-Sphere}
We analyse the geometry of hyper-representations $\mathbf{z}$. 
The space of hyper-representations is bounded to a high dimensional box by a tanh activation. Surprisingly, hyper-representations do not populate the entire space, but sections on a shell of a high-dimensional sphere.
Figure \ref{fig:hyper_sphere_z} shows the distribution of the norm of the embeddings of the MNIST zoo. 
All embeddings are distributed on a small band between length 10 and 12, therefore they must populate the shell of a hyper-sphere.
In Figure \ref{fig:hyper_sphere_cos} we investigate pairwise cosine distances between the embeddings of the MNIST zoo. The majority of the embeddings populate the region between 0.6 and 0.8. 
The outliers around 1.0 are embeddings of the same model at different epochs. 
This indicates that models are not entirely orthogonal, but mutually equally far apart, populating a section of the shell of the hyper-sphere. 
While hyper-spheres are commonly found in embeddings of contrastive  learning~\citep{jingUnderstandingDimensionalCollapse2021}, in our experiments hyper-spheres form even without a contrastive loss. Properties of the models embedded on that hyper-sphere can be predicted from hyper-representations, therefore the topology on the sphere appears to encode model properties. \looseness-1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht!]
\begin{minipage}[t]{0.49\textwidth}
\begin{center}
\includegraphics[trim=0in 0in 0in 0in, clip, width=0.9\linewidth]{figures/hyper_representations_l2norm.png}
\vskip -0.1in
\caption{Distributions of $\ell_2$ norm of hyper-representations $\mathbf{z}$ of the MNIST zoo.}
\label{fig:hyper_sphere_z}    
\end{center}
\end{minipage}
\hskip 0.2cm
\begin{minipage}[t]{0.49\textwidth}
\begin{center}
\includegraphics[trim=0in 0in 0in 0in, clip, width=0.9\linewidth]{figures/hyper_representations_cosine_distance.png}
\vskip -0.1in
\caption{Distributions of pairwise cosine distance of hyper-representations $\mathbf{z}$ of the MNIST zoo.}
\label{fig:hyper_sphere_cos}    
\end{center}
\end{minipage}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Distributions of Dimensions of Embeddings in Hyper-Representation Encode Properties}
Previous work showed that linear probing from hyper-representations accurately predicts i.e. model accuracy. 
In these linear probes, the individual $z$ dimensions each linearly contribute to accuracy predictions. 
This allows us investigate $z$ dimensions independently.
Figure \ref{fig:z_wise_distribution} shows examples for the distribution of selected individual dimensions of hyper-representations $\mathbf{z}$. On the left are the distribution of the entire population, on the right of the top 30 \% performing models.
The individual dimensions show different types of distributions, with different modes. Most have a zero mean and span 3/4 of the available range, but some collapse to either $-1$ or $1$. 
Further, the distributions also differ in at least some dimension between the entire population, and the better performing split of the population.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht!]
\begin{minipage}[t]{0.99\textwidth}
\begin{center}
\includegraphics[trim=0mm 5.3mm 0mm 0mm, clip, width=1.0\linewidth]{figures/z_wise_distro_choice.png}
\vskip 0mm
\caption{Distributions of individual dimensions of hyper-representations $\mathbf{z}$ of the MNIST zoo. In {\color{cyan}blue} is the distribution of all samples, in {\color{orange}orange} the subset of the 30 \% best samples.}
\label{fig:z_wise_distribution}    
\end{center}
\end{minipage}
% \vskip -0.15in
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Generalization Capabilities of Hyper-Representations to Diverse Model Zoos}
There are certain architectural changes such as adding/removing/changing pooling layers and nonlinearity that do not change the number of parameters (the dimensionality of the input/output required by our approach). These changes as well as changes of hyperparameters used to train models in a zoo may drastically alter the distribution of weights and pose a challenge to the proposed approach. Modern neural networks (ResNet, MobileNet, EfficientNet, etc.) are often trained with very different hyperparameters. With the experiment below, we investigate the generalization capabilities of hyper-representations to suchchanges, which might be important for modern large-scale settings as well.

\textbf{Setup:} We experimentally evaluate generalizability of the proposed approach on models trained with a different choice of nonlinearity or other hyperparameters with two experiments (a and b). To that end, in addition to the original SVHN test zoo (zoo 1), we use two more diverse SVHN zoos (zoo 2 and zoo 3). In zoo 2, in addition to random seed, models differ in the activation (tanh, relu, gelu, sigmoid), l2-regularization (0, 0.001, 0.1) and dropout (0,0.3,0.5). In zoo 3 (extending zoo 2), we increase the diversity further by additionally varying the initialization method (uniform, normal, kaiming-uniform, kaiming-normal) and the learning rate (0.0001, 0.001, 0.01).

\textbf{Experiment (a):} We first evaluate our original encoder-decoder trained on a model zoo varying in random seed only. For evaluation, we pass the test splits of zoo 2 and zoo 3 through the encoder-decoder. We measure the reconstruction $R^2$ score of the original encoder-decoder on the diverse test zoos. \\
\textbf{Results:} Our results (Table \ref{tab:hyper_rep_generalization}) indicate that our original encoder-decoder can still encode and decode weights even in such a challenging setting, although there is an expected drop of performance.

\textbf{Experiment (a):} We next evaluate if hyper-representations can be trained on diverse zoos. For this experiment, we train a hyper-representation on the train split of zoo 3. With this, we aim to show that training hyper-representations on diverse zoos improves generalization capabilities further. \\
\textbf{Results:} Our results show that training on diverse zoos is a much more difficult task to optimize, hence the reconstruction on the original zoo degrades. It nonetheless improves the reconstruction results on the test split of the diverse zoos 2 and 3. This indicates that varying seeds and hyperparameters may be different aspects of complexity that need to be considered. 
\begin{table}[ht!]
\begin{minipage}[t]{0.98\textwidth}
\centering
{\small
\caption{Generalizability of hyper-representations towards more diverse model zoo configurations (measured as the reconstruction score, higher is better).}
\begin{tabular}{@{}rccc@{}}
% \begin{tabularx}{rccc}
\toprule
\textbf{Training zoo}    & \textbf{Test zoo 1: original}  & \textbf{Test zoo 2: vary activation} & \textbf{Test zoo 3: vary hyperparameters} \\
\midrule
Original        & 81.9\%               & 45.7\%                                              & 38.9\%                           \\
Diverse (zoo 3) & 25.8\%               & 89.1\%                                              & 75.6\%                          \\
\bottomrule
\end{tabular}
% \end{tabularx}
}
\label{tab:hyper_rep_generalization}
\end{minipage}
\vspace{-.2in}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SAMPLING METHODS
\newpage
\section{Sampling Methods }
\label{app:sampling}
%
\subsection{VAE}
A common extension of the autoencoder of \citep{schurholtSelfSupervisedRepresentationLearning2021} to enable sampling from its latent representation is to make the autoencoder variational~\citep{kingmaAutoEncodingVariationalBayes2013}.
% Another common choice for generative representation learning is generative adversarial networks (GANs)~\citep{goodfellowGenerativeAdversarialNets}.
In our experiments, VAEs could not be trained to satisfactory reconstruct model weights without unweighting the KL-divergence to insignificance essentially making it deterministic as in~\citep{schurholtSelfSupervisedRepresentationLearning2021}. 
% Training GANs is also challenging, b \S~\ref{sec:gan}.
%%% Old Version
% A natural choice for generative representation learning are Variational autoencoders (VAEs)~\citep{kingmaAutoEncodingVariationalBayes2014}. 
% In our experiments, VAEs could not be trained to satisfactory reconstruct model without unweighting the KL-divergence to insignificance. 
Empirically, embeddings in hyper-representations are mapped on the shell of a sphere (see Section \ref{app:analysis}) and leave the inside of the sphere entirely empty. 
On the other hand, a gaussian prior allocates most of the probability mass near the center of the sphere. 
It therefore appears plausible that the two may be incompatible.
That issue of non-compatible priors is well known.% and solutions to overcome it have been proposed, e.g., VQ-VAEs~\citep{oordNeuralDiscreteRepresentation2018}. 
~\citep{ghoshVariationalDeterministicAutoencoders2020} find that regularizing embeddings and decoder yields equally smooth representation spaces as VAEs without restrictions to specific priors.
During training of hyper-representations, both encoder and decoder are regularized with a small $\ell_2$ penalty. Further, dropout is applied throughout the autoencoder, which servers as another regularizer and adds blurryness to the embeddings. The combination of dropout, the erasing augmentation and the contrastive loss further regularizes the hyper-representation space. 
In all our sampling methods, we draw samples from probability distributions, which effectively disconnects the drawn samples from training embeddings.
\looseness-1

\subsection{Latent Space GAN Details}
The generator and discriminator of our GAN consist of four fully-connected layers interleaved with ReLU nonlinearities. The same architecture and training hyperparameters are used for all experiments.
The generator's input is a Gaussian noise $\mathbf{n}^*$ of dimensionality $d=16$, the hidden dimensionalities are 128, 256 and 512, and the output dimensionality is equal to the hyper-representation length $D$.
The discriminator's input is $D$-dimensional, the hidden dimensionalities are 1024, 512 and 256, and the output dimensionality is a scalar denoting either a real or fake sample.
The discriminator is regularized with Spectral Norm~\cite{miyatoSpectralNormalizationGenerative2018}. The discriminator and generator are trained for 1000 epochs and batch size 32 using Adam with a two time-scale update rule~\cite{heusel2017gans}: learning rate is 1e-4 for the generator and 2e-4 for the discriminator.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\FloatBarrier
\newpage
\section{Full Experiment Results }
\label{app:results}

% \newpage
\subsection{Digit Domain}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[ht!]
\begin{minipage}[t]{0.98\textwidth}
\centering
{\small
\caption{Accuracy of sampled models: median and 95\% confidence intervals. On the main diagonal are in-dataset experiments, otherwise transfer-learning from source to target. Bold numbers highlight the best source-to-target results. N/A enotes cases, in which the boot-strapped CI on the median could not be computed.}
\begin{tabularx}{0.6\linewidth}{lccc}
\toprule
Population           & Source                 & \multicolumn{2}{c}{Target}                    \\
\cmidrule(rl){1-1} \cmidrule(rl){2-2}  \cmidrule(rl){3-4} 
                     & \multicolumn{1}{l}{}   & MNIST                 & SVHN                  \\
 \cmidrule(rl){3-3} \cmidrule(rl){4-4}  
$B_T$                & \multirow{8}{*}{MNIST} & 91.1 {[}91.1, 91.2{]} & 72.3 {[}72.0, 72.4{]} \\
\cmidrule(rl){1-1} \cmidrule(rl){2-2}  \cmidrule(rl){3-4} 
$B_F$                &                        & 91.2 {[}91.0, 91.3{]} & 76.2 {[}75.8, 76.5{]} \\
$S_{\text{KDE}}$     &                        & 92.3 {[}92.1, 92.8{]} & 76.7 {[}76.2, 77.0{]} \\
$S_{\text{KDE}30}$   &                        & 93.1 {[}92.9, 93.4{]} & 77.2 {[}76.8, 77.6{]} \\
$S_{\text{Neigh}}$   &                        & 93.4 {[}93.2, 93.5{]} & 76.8 {[}76.4, 77.1{]} \\
$S_{\text{Neigh}30}$ &                        & \textbf{94.0 {[}93.8, 94.1{]}} & \textbf{77.0 {[}76.3, 77.4{]}} \\
$S_{\text{GAN}}$     &                        & 93.5 {[}93.3, 93.6{]} & 76.9 {[}76.6, 77.6{]} \\
$S_{\text{GAN}30}$   &                        & 93.9 {[}93.5, 93.9{]} & 76.5 {[}76.3, 76.8{]} \\
\cmidrule(rl){1-1} \cmidrule(rl){2-2}  \cmidrule(rl){3-4} 
$B_F$                & \multirow{7}{*}{SVHN}  & 95.1 {[}95.0, 95.3{]} & 73.2 {[}72.8, 73.4{]} \\
$S_{\text{KDE}}$     &                        & 95.1 N/A              & 73.0 {[}72.6, 73.3{]} \\
$S_{\text{KDE}30}$   &                        & 95.5 N/A              & 74.2 {[}73.9, 74.5{]} \\
$S_{\text{Neigh}}$   &                        & \textbf{97.2 {[}97.0, 97.3{]}} & \textbf{78.1 {[}77.9, 78.2{]}} \\
$S_{\text{Neigh}30}$ &                        & 95.5 {[}95.4, 95.7{]} & 76.5 {[}76.3, 76.7{]} \\
$S_{\text{GAN}}$     &                        & 94.3 {[}94.1, 94.6{]} & 74.5 {[}74.0, 74.9{]} \\
$S_{\text{GAN}30}$   &                        & 94.9 {[}94.8, 95.1{]} & 75.3 {[}75.0, 75.6   \\
\bottomrule
\end{tabularx}
}
\label{tab:accuracy_digits}
\end{minipage}
\vspace{-.2in}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[ht!]
\begin{minipage}[t]{0.98\textwidth}
\centering
{\small
\caption{Mann-Whitney U test of Samples S vs Baselines B: p-value and CLES (Common Language Effect Size). p-values indicate the probability of the samples of two groups originating from the same distribution. CLES=0.5 indicates no effect, CLES=1.0 a strong positive, CLES=0.0 a strong negative effect.
As the results indicate, both proposed sampling methods are almost always statistically significantly better than the two baselines. Further, their effect is often very strong. 
}
% \begin{table}[]
\begin{tabularx}{0.6\linewidth}{@{}lccc@{}}
\toprule
Population Pairs                 & Source                  & \multicolumn{2}{c}{Target}                            \\
\cmidrule(rl){1-1} \cmidrule(rl){2-2}  \cmidrule(rl){3-4} 
                                 &                         & MNIST                     & SVHN                      \\
 \cmidrule(rl){3-3} \cmidrule(rl){4-4}
$S_{\text{KDE}}$ vs. $B_{T}$     & \multirow{12}{*}{MNIST} & \textbf{2.1e-18 | 0.8701} & \textbf{5.2e-27 | 0.9551} \\
$S_{\text{KDE}}$ vs. $B_{F}$     &                         & \textbf{0.0e+00 | 0.8639} & \textbf{1.1e-01 | 0.5920} \\
$S_{\text{KDE}30}$ vs. $B_{T}$   &                         & \textbf{7.0e-27 | 0.9539} & \textbf{2.5e-29 | 0.9754} \\
$S_{\text{KDE}30}$ vs. $B_{F}$   &                         & \textbf{6.9e-22 | 0.9545} & \textbf{1.7e-04 | 0.7180} \\
$S_{\text{Neigh}}$ vs. $B_{T}$   &                         & \textbf{1.5e-30 | 0.9857} & \textbf{6.6e-31 | 0.9888} \\
$S_{\text{Neigh}}$ vs. $B_{F}$   &                         & \textbf{4.5e-25 | 0.9889} & \textbf{5.2e-03 | 0.6622} \\
$S_{\text{Neigh}30}$ vs. $B_{T}$ &                         & \textbf{1.7e-35 | 0.9987} & \textbf{1.3e-29 | 0.9778} \\
$S_{\text{Neigh}30}$ vs. $B_{F}$ &                         & \textbf{3.1e-28 | 0.9994} & \textbf{1.4e-02 | 0.6426} \\
$S_{\text{GAN}}$ vs. $B_{T}$     &                         & \textbf{7.6e-31 | 0.9883} & \textbf{8.0e-25 | 0.9351} \\
$S_{\text{GAN}}$ vs. $B_{F}$     &                         & \textbf{3.0e-25 | 0.9907} & \textbf{7.8e-03 | 0.6546} \\
$S_{\text{GAN}30}$ vs. $B_{T}$   &                         & \textbf{1.1e-31 | 0.9953} & \textbf{2.1e-26 | 0.9496} \\
$S_{\text{GAN}30}$ vs. $B_{F}$   &                         & \textbf{6.8e-26 | 0.9973} & \textbf{4.9e-02 | 0.6144} \\
\cmidrule(rl){1-1} \cmidrule(rl){2-2}  \cmidrule(rl){3-4} 
$S_{\text{KDE}}$ vs. $B_{T}$     & \multirow{12}{*}{SVHN}  & \textbf{6.1e-79 | 0.9943} & \textbf{1.1e-04 | 0.6006} \\
$S_{\text{KDE}}$ vs. $B_{F}$     &                         & 7.8e-01 | 0.4904          & 3.8e-01 | 0.4704          \\
$S_{\text{KDE}30}$ vs. $B_{T}$   &                         & \textbf{1.7e-82 | 1.0000} & \textbf{1.6e-30 | 0.7985} \\
$S_{\text{KDE}30}$ vs. $B_{F}$   &                         & \textbf{0.0e+00 | 0.7292} & \textbf{3.0e-08 | 0.6850} \\
$S_{\text{Neigh}}$ vs. $B_{T}$   &                         & \textbf{2.9e-78 | 0.9867} & \textbf{8.6e-80 | 0.9916} \\
$S_{\text{Neigh}}$ vs. $B_{F}$   &                         & \textbf{2.8e-44 | 0.9661} & \textbf{1.8e-47 | 0.9833} \\
$S_{\text{Neigh}30}$ vs. $B_{T}$ &                         & \textbf{1.7e-82 | 1.0000} & \textbf{4.7e-76 | 0.9797} \\
$S_{\text{Neigh}30}$ vs. $B_{F}$ &                         & \textbf{8.2e-08 | 0.6791} & \textbf{1.7e-42 | 0.9563} \\
$S_{\text{GAN}}$ vs. $B_{T}$     &                         & \textbf{1.2e-31 | 0.9948} & \textbf{0.0e+00 | 0.8140} \\
$S_{\text{GAN}}$ vs. $B_{F}$     &                         & 1.5e-07 | 0.2517          & \textbf{7.5e-06 | 0.7118} \\
$S_{\text{GAN}30}$ vs. $B_{T}$   &                         & \textbf{4.2e-32 | 0.9987} & \textbf{6.7e-22 | 0.9067} \\
$S_{\text{GAN}30}$ vs. $B_{F}$   &                         & 3.6e-01 | 0.4565          & \textbf{0.0e+00 | 0.8335} \\
\bottomrule
\end{tabularx}
% \end{table}
}
\label{tab:test_digits}
\end{minipage}
\vspace{-.2in}
\end{table}


% \subsubsection{Source: MNIST}

\begin{figure*}[ht!]
\centering
\begin{minipage}[t]{0.42\textwidth}
\begin{center}
\includegraphics[angle=90,origin=c, trim=0in 0in 0in 0in, clip, width=1.00\linewidth]{figures/boxplots/MNIST_to_MNIST_boxplot.png}
% \vskip -0.1in
\caption{MNIST in-dataset experiment: accuracy over epochs. 
% The first 25 epochs of $B_T$ are wrapped for conciseness.
Boxes indicate quintiles 25 to 75. }
\label{fig:sampling_boxplot_mnist_mnist}    
\end{center}
\end{minipage}
\hskip 2mm
\begin{minipage}[t]{0.42\textwidth}
\begin{center}
\includegraphics[angle=90,origin=c, trim=0in 0in 0in 0in, clip, width=1.00\linewidth]{figures/boxplots/MNIST_to_SVHN_boxplot.png}
\caption{MNIST to SVHN transfer learning experiment: accuracy over epochs. Boxes indicate quintiles 25 to 75. }
\label{fig:sampling_boxplot_mnist_svhn}    
\end{center}
\end{minipage}
% \hskip 2mm
% \begin{minipage}[t]{0.32\textwidth}
% \begin{center}
% \includegraphics[angle=90,origin=c, trim=0in 0in 0in 0in, clip, width=1.00\linewidth]{figures/boxplots/MNIST_to_USPS_boxplot.png}
% % \vskip -0.1in
% \caption{MNIST to USPS transfer learning experiment: accuracy over epochs. Boxes indicate quintiles 25 to 75. Both sampling methods overtake $B_F$ after few epochs, $B_T$ achieves highest performance.}
% \label{fig:sampling_boxplot_mnist_usps}    
% \end{center}
% \end{minipage}
\end{figure*}



% \subsubsection{Source: SVHN}
\begin{figure*}[ht!]
\centering
\begin{minipage}[t]{0.42\textwidth}
\begin{center}
\includegraphics[angle=90,origin=c, trim=0in 0in 0in 0in, clip, width=1.00\linewidth]{figures/boxplots/SVHN_to_SVHN_boxplot.png}
% \vskip -0.1in
\caption{SVHN in-dataset experiment: accuracy over epochs. 
% The first 25 epochs of $B_T$ are wrapped for conciseness. 
Boxes indicate quintiles 25 to 75. }
\label{fig:sampling_boxplot_svhn_svhn}    
\end{center}
\end{minipage}
\hskip 2mm
\begin{minipage}[t]{0.42\textwidth}
\begin{center}
\includegraphics[angle=90,origin=c, trim=0in 0in 0in 0in, clip, width=1.00\linewidth]{figures/boxplots/SVHN_to_MNIST_boxplot.png}
\caption{SVHN to MNIST transfer learning experiment: accuracy over epochs. Boxes indicate quintiles 25 to 75.}
\label{fig:sampling_boxplot_svhn_mnist}    
\end{center}
\end{minipage}
\hskip 2mm
\end{figure*}

\FloatBarrier
\newpage
\subsection{Natural Images Domain}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[ht!]
\begin{minipage}[t]{0.98\textwidth}
\centering
{\small
\caption{Accuracy of sampled models: median and 95\% confidence intervals. On the main diagonal are in-dataset experiments, otherwise transfer-learning from source to target. Bold numbers highlight the best source-to-target results. N/A enotes cases, in which the boot-strapped CI on the median could not be computed.}
\begin{tabularx}{0.6\linewidth}{lcll}
\toprule
Population           & Source                    & \multicolumn{2}{c}{Target}                                      \\
\cmidrule(rl){1-1} \cmidrule(rl){2-2}  \cmidrule(rl){3-4} 
                     & \multicolumn{1}{l}{}      & \multicolumn{1}{c}{CIFAR-10}   & \multicolumn{1}{c}{STL-10}     \\
 \cmidrule(rl){3-3} \cmidrule(rl){4-4}  
$B_T$                & \multirow{8}{*}{CIFAR-10} & 49.0 {[}48.9, 49.0{]}          & 39.0 {[}38.9, 39.1{]}          \\
\cmidrule(rl){1-1} \cmidrule(rl){2-2}  \cmidrule(rl){3-4} 
$B_F$                &                           & 48.6 {[}48.3, 48.7{]}          & \textbf{42.8 {[}42.5, 42.9{]}} \\
$S_{\text{KDE}}$     &                           & 48.3 {[}48.1, 48.4{]}          & 40.7 {[}40.3, 40.9{]}          \\
$S_{\text{KDE}30}$   &                           & \textbf{48.7 {[}48.4, 48.8{]}} & 41.3 {[}40.9, 41.5{]}          \\
$S_{\text{Neigh}}$   &                           & 45.6 {[}44.9, 46.0{]}          & 36.7 {[}35.8, 37.4{]}          \\
$S_{\text{Neigh}30}$ &                           & 46.2 {[}45.8, 46.4{]}          & 37.9 {[}37.3, 38.2{]}          \\
$S_{\text{GAN}}$     &                           & 46.0 N/A                       & 38.6 {[}38.1, 39.0{]}          \\
$S_{\text{GAN}30}$   &                           & 47.0 {[}46.5, 47.2{]}          & 38.6 {[}38.2, 39.1{]}          \\
\cmidrule(rl){1-1} \cmidrule(rl){2-2}  \cmidrule(rl){3-4} 
$B_F$                & \multirow{7}{*}{STL-10}   & \textbf{49.3 {[}49.0, 49.4{]}} & \textbf{39.5 {[}38.9, 39.7{]}} \\
$S_{\text{KDE}}$     &                           & 48.6 {[}48.4, 48.9{]}          & 37.3 {[}37.0, 37.8{]}          \\
$S_{\text{KDE}30}$   &                           & 48.8 {[}48.4, 49.2{]}          & 38.3 {[}37.9, 38.4{]}          \\
$S_{\text{Neigh}}$   &                           & 10.0 N/A                       & 28.3 {[}26.8, 29.1{]}          \\
$S_{\text{Neigh}30}$ &                           & 49.0 {[}48.5, 49.1{]}          & 37.8 {[}37.6, 38.2{]}          \\
$S_{\text{GAN}}$     &                           & 49.0 {[}48.6, 49.4{]}          & 38.5 {[}37.9, 38.9{]}          \\
$S_{\text{GAN}30}$   &                           & 48.8 {[}48.5, 49.1{]}          & 37.9 N/A                      \\
\bottomrule
\end{tabularx}
}
\label{tab:accuracy_natural_images}
\end{minipage}
\vspace{-.2in}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[ht!]
\centering
\begin{minipage}[t]{0.98\textwidth}
\centering
{\small
\caption{Mann-Whitney U test of Samples S vs Baselines B: p-value and CLES (Common Language Effect Size). p-values indicate the probability of the samples of two groups originating from the same distribution. CLES=0.5 indicates no effect, CLES=1.0 a strong positive, CLES=0.0 a strong negative effect.
}
% \begin{table}[]
\begin{tabularx}{0.6\linewidth}{@{}lccc@{}}
\toprule
Population Pairs                 & Source                     & \multicolumn{2}{c}{Target}                            \\
\cmidrule(rl){1-1} \cmidrule(rl){2-2}  \cmidrule(rl){3-4} 
                                 &                            & CIFAR-10                  & STL-10                    \\
 \cmidrule(rl){3-3} \cmidrule(rl){4-4}
$S_{\text{KDE}}$ vs. $B_{T}$     & \multirow{12}{*}{CIFAR-10} & 1.5e-06 | 0.2966          & \textbf{7.4e-19 | 0.8750} \\
$S_{\text{KDE}}$ vs. $B_{F}$     &                            & 3.7e-02 | 0.4014          & 1.7e-18 | 0.0849          \\
$S_{\text{KDE}30}$ vs. $B_{T}$   &                            & 3.6e-02 | 0.4114          & \textbf{4.8e-25 | 0.9371} \\
$S_{\text{KDE}30}$ vs. $B_{F}$   &                            & \textbf{2.9e-01 | 0.5498} & 0.0e+00 | 0.1266          \\
$S_{\text{Neigh}}$ vs. $B_{T}$   &                            & 5.7e-28 | 0.0364          & 7.4e-18 | 0.1359          \\
$S_{\text{Neigh}}$ vs. $B_{F}$   &                            & 3.1e-22 | 0.0413          & 7.1e-26 | 0.0024          \\
$S_{\text{Neigh}30}$ vs. $B_{T}$ &                            & 3.5e-25 | 0.0616          & 2.0e-07 | 0.2800          \\
$S_{\text{Neigh}30}$ vs. $B_{F}$ &                            & 2.2e-19 | 0.0741          & 3.0e-25 | 0.0089          \\
$S_{\text{GAN}}$ vs. $B_{T}$     &                            & 6.6e-25 | 0.0642          & 6.6e-02 | 0.4223          \\
$S_{\text{GAN}}$ vs. $B_{F}$     &                            & 2.8e-19 | 0.0754          & 1.0e-24 | 0.0145          \\
$S_{\text{GAN}30}$ vs. $B_{T}$   &                            & 2.1e-21 | 0.0983          & 1.1e-02 | 0.3928          \\
$S_{\text{GAN}30}$ vs. $B_{F}$   &                            & 8.8e-16 | 0.1195          & 2.7e-25 | 0.0084          \\
\cmidrule(rl){1-1} \cmidrule(rl){2-2}  \cmidrule(rl){3-4} 
$S_{\text{KDE}}$ vs. $B_{T}$     & \multirow{12}{*}{STL-10}   & 1.3e-01 | 0.4362          & 0.0e+00 | 0.1730          \\
$S_{\text{KDE}}$ vs. $B_{F}$     &                            & 6.9e-04 | 0.3028          & 6.0e-10 | 0.1404          \\
$S_{\text{KDE}30}$ vs. $B_{T}$   &                            & 6.1e-01 | 0.4783          & 1.2e-06 | 0.2948          \\
$S_{\text{KDE}30}$ vs. $B_{F}$   &                            & 1.1e-02 | 0.3528          & 9.1e-06 | 0.2424          \\
$S_{\text{Neigh}}$ vs. $B_{T}$   &                            & 2.9e-32 | 0.0000          & 3.0e-32 | 0.0000          \\
$S_{\text{Neigh}}$ vs. $B_{F}$   &                            & 3.3e-20 | 0.0000          & 7.1e-18 | 0.0000          \\
$S_{\text{Neigh}30}$ vs. $B_{T}$ &                            & 1.0e+00 | 0.5000          & 4.3e-09 | 0.2517          \\
$S_{\text{Neigh}30}$ vs. $B_{F}$ &                            & 2.1e-02 | 0.3654          & 5.4e-07 | 0.2090          \\
$S_{\text{GAN}}$ vs. $B_{T}$     &                            & 3.2e-01 | 0.5418          & 2.0e-04 | 0.3427          \\
$S_{\text{GAN}}$ vs. $B_{F}$     &                            & 2.7e-01 | 0.4360          & 2.4e-04 | 0.2864          \\
$S_{\text{GAN}30}$ vs. $B_{T}$   &                            & 6.2e-01 | 0.4788          & 5.4e-07 | 0.2880          \\
$S_{\text{GAN}30}$ vs. $B_{F}$   &                            & 1.2e-02 | 0.3532          & 4.6e-06 | 0.2340         \\
\bottomrule
\end{tabularx}
}
\label{tab:test_natural_images}
\end{minipage}
\vspace{-.2in}
\end{table}

% \subsubsection{Source: CIFAR-10}

\begin{figure*}[ht!]
\centering
\begin{minipage}[t]{0.42\textwidth}
\begin{center}
\includegraphics[angle=90,origin=c, trim=0in 0in 0in 0in, clip, width=1.00\linewidth]{figures/boxplots/CIFAR-10_to_CIFAR-10_boxplot.png}
% \vskip -0.1in
\caption{CIFAR-10 in-dataset experiment: accuracy over epochs. 
% The first 25 epochs of $B_T$ are wrapped for conciseness. 
Boxes indicate quintiles 25 to 75.}
\label{fig:sampling_boxplot_cifar_cifar}    
\end{center}
\end{minipage}
\hskip 2mm
\begin{minipage}[t]{0.42\textwidth}
\begin{center}
\includegraphics[angle=90,origin=c, trim=0in 0in 0in 0in, clip, width=1.00\linewidth]{figures/boxplots/CIFAR-10_to_STL-10_boxplot.png}
% \vskip -0.1in
\caption{CIFAR-10 to STL-10 transfer learning experiment: accuracy over epochs. Boxes indicate quintiles 25 to 75.}
\label{fig:sampling_boxplot_cifar_stl}    
\end{center}
\end{minipage}

\end{figure*}

% \subsubsection{Source: STL-10}

\begin{figure*}[ht!]
\centering
\begin{minipage}[t]{0.42\textwidth}
\begin{center}
\includegraphics[angle=90,origin=c, trim=0in 0in 0in 0in, clip, width=1.00\linewidth]{figures/boxplots/STL-10_to_STL-10_boxplot.png}
% \vskip -0.1in
\caption{STL-10 in-dataset experiment: accuracy over epochs. 
% The first 25 epochs of $B_T$ are wrapped for conciseness. 
Boxes indicate quintiles 25 to 75. }
\label{fig:sampling_boxplot_stl_stl}    
\end{center}
\end{minipage}
\hskip 2mm
\begin{minipage}[t]{0.42\textwidth}
\begin{center}
\includegraphics[angle=90,origin=c, trim=0in 0in 0in 0in, clip, width=1.00\linewidth]{figures/boxplots/STL-10_to_CIFAR-10_boxplot.png}
% \vskip -0.1in
\caption{STL-10 to CIFAR-10 transfer learning experiment: accuracy over epochs. Boxes indicate quintiles 25 to 75.}
\label{fig:sampling_boxplot_stl_cifar}    
\end{center}
\end{minipage}

\end{figure*}

